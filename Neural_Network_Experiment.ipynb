{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xt-ig_zOm15"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import os,sys,re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import statistics\n",
        "import time\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import random\n",
        "from scipy.stats import spearmanr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dh5FyfyQavF"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  # 'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, data, labels):\n",
        "        'Initialization'\n",
        "        self.labels = labels\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        X = self.data[index]\n",
        "        y = self.labels[index]\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNSJRpqZQvsd"
      },
      "outputs": [],
      "source": [
        "class NN_Model(nn.Module):\n",
        "  \n",
        "    def __init__(self, num_labels, config=None, device=torch.device(\"cuda:0\")):\n",
        "        super(NN_Model, self).__init__()\n",
        "        self.dense1 = nn.Linear(in_features=41, out_features=10) #Add ReLu in forward loop\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.dense2 = nn.Linear(in_features=10, out_features=num_labels) #Add softmax in forward loop\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, inputs, attention_mask=None, labels=None):\n",
        "\n",
        "        X = inputs.to(self.device)\n",
        "        X = F.relu(self.dense1(X.float()))\n",
        "        X = self.dropout(X)\n",
        "        X = F.log_softmax(self.dense2(X))\n",
        "        return X\n",
        "\n",
        "def save_models(epochs, model):\n",
        "    torch.save(model.state_dict(), \"bert_model_fold_{}.h5\".format(epochs))\n",
        "    print(\"Checkpoint Saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKPQ2jkyqiav"
      },
      "outputs": [],
      "source": [
        "def compute_correlation(original_data, label_list, fake_data):\n",
        "\n",
        "  corr_list = []\n",
        "\n",
        "  majority_original = []\n",
        "\n",
        "  for i in range(len(label_list)):\n",
        "    if int(label_list[i])==1:\n",
        "      majority_original.append(original_data[i])\n",
        "  \n",
        "  majority_original = np.array(majority_original)\n",
        "  mean_majority_vec = np.mean(majority_original, axis=0)\n",
        "\n",
        "  sum_corr = 0\n",
        "  for i in range(fake_data.shape[0]):\n",
        "    corr, _ = spearmanr(mean_majority_vec, fake_data[i])\n",
        "    sum_corr += corr\n",
        "  return sum_corr/float(fake_data.shape[0])\n",
        "\n",
        "def compare_datasets(original_dataset, fake_dataset):\n",
        "  \n",
        "  column_list = []\n",
        "  original_index = {1}\n",
        "  fake_index = {original_dataset.shape[0]+1}\n",
        "\n",
        "  for i in range(1, original_dataset.shape[0]):\n",
        "    original_index.add(i+1)\n",
        "  \n",
        "  for i in range(1, fake_dataset.shape[0]):\n",
        "    fake_index.add(original_dataset.shape[0]+i+1)\n",
        "  \n",
        "  for i in range(41):\n",
        "    column_list.append(\"OC_\"+ str(i+1))\n",
        "\n",
        "  original_df = pd.DataFrame(original_dataset, columns=column_list, index=original_index)\n",
        "  original_df.drop_duplicates()\n",
        "  fake_df = pd.DataFrame(fake_dataset, columns=column_list, index=fake_index)\n",
        "  fake_df.drop_duplicates()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  table_evaluator = TableEvaluator(original_df, fake_df)\n",
        "  table_evaluator.visual_evaluation()\n",
        "\n",
        "def apply_gaussian_noise(data_list, label_list):\n",
        "  original_dataset = np.copy(data_list)\n",
        "\n",
        "  index_list = []\n",
        "  for i in range(len(label_list)):\n",
        "    if int(label_list[i])==1:\n",
        "      index_list.append(i)\n",
        "  \n",
        "  for i in range(len(index_list)-1):\n",
        "    for j in range(i+1, len(index_list)):\n",
        "      \n",
        "      rand_el = random.choice([i, j])\n",
        "      max_num = np.max(data_list[rand_el])\n",
        "      min_num = np.min(data_list[rand_el])\n",
        "      if abs(min_num) >= max_num:\n",
        "        gauss = np.random.normal(0,(float(max_num)/float(2)),data_list[i].shape)\n",
        "      else:\n",
        "        gauss = np.random.normal(0,(float(abs(min_num))/float(2)),data_list[i].shape)\n",
        "      #print(gauss)\n",
        "\n",
        "      new_data_row = data_list[rand_el] + gauss\n",
        "\n",
        "      new_data_row = np.reshape(new_data_row, (1, new_data_row.shape[0]))\n",
        "\n",
        "      if i==0 and j==1:\n",
        "        fake_dataset = np.copy(new_data_row)\n",
        "      else:\n",
        "        fake_dataset = np.append(fake_dataset, new_data_row , axis=0)  \n",
        "        \n",
        "\n",
        "      data_list = np.append(data_list, new_data_row , axis=0)\n",
        "      label_list = np.append(label_list, 1)        \n",
        "    \n",
        "    return data_list, label_list, fake_dataset\n",
        "\n",
        "\n",
        "def apply_majority_oversampling(data_list, label_list):\n",
        "\n",
        "    original_dataset = np.copy(data_list)\n",
        "\n",
        "    index_list = []\n",
        "    for i in range(len(label_list)):\n",
        "      if int(label_list[i])==1:\n",
        "        index_list.append(i)\n",
        "    \n",
        "    for i in range(len(index_list)-1):\n",
        "      for j in range(i+1, len(index_list)):\n",
        "        ratio = random.random()\n",
        "        while ratio==0 or ratio==1:\n",
        "          ratio = random.random()\n",
        "\n",
        "        new_data_row = ratio*data_list[i] + (1-ratio)*data_list[j]\n",
        "\n",
        "        new_data_row = np.reshape(new_data_row, (1, new_data_row.shape[0]))\n",
        "\n",
        "        if i==0 and j==1:\n",
        "          fake_dataset = np.copy(new_data_row)\n",
        "        else:\n",
        "          fake_dataset = np.append(fake_dataset, new_data_row , axis=0)  \n",
        "        \n",
        "\n",
        "        data_list = np.append(data_list, new_data_row , axis=0)\n",
        "        label_list = np.append(label_list, 1)        \n",
        "\n",
        "    \n",
        "    return data_list, label_list, fake_dataset\n",
        "\n",
        "\n",
        "\n",
        "def apply_smote(data_list, label_list):\n",
        "\n",
        "    #print(\"Before Count: \", Counter(label_list))\n",
        "    \n",
        "    # Apply majority oversampling or gaussina noise\n",
        "    original_data_list = np.copy(data_list)\n",
        "    original_label_list = copy.deepcopy(label_list)\n",
        "\n",
        "    data_list, label_list, fake_list = apply_majority_oversampling(data_list, label_list)\n",
        "    #data_list, label_list, fake_list = apply_gaussian_noise(data_list, label_list)\n",
        "    \n",
        "    #print(data_list.shape)\n",
        "    # Apply SMOTE\n",
        "    transformed_data_list = np.copy(data_list)\n",
        "    #print(\"After Majority Oversampling Count: \", Counter(label_list))\n",
        "    orig_shape = transformed_data_list.shape\n",
        "    transformed_label_list = []\n",
        "    '''for i in range(0,transformed_data_list.shape[0]):\n",
        "            for j in range(12):\n",
        "                transformed_label_list.append(int(label_list[i]))'''\n",
        "    \n",
        "    for i in range(0,transformed_data_list.shape[0]):\n",
        "          transformed_label_list.append(int(label_list[i]))    \n",
        "\n",
        "    #print(\"Original Shape: \", orig_shape)\n",
        "    \n",
        "\n",
        "\n",
        "    oversample = SMOTE(k_neighbors=1)\n",
        "    transformed_data_list, transformed_label_list = oversample.fit_resample(transformed_data_list, transformed_label_list)\n",
        "    #print(len(transformed_label_list))\n",
        "    added_num = int(transformed_data_list.shape[0]) - int(data_list.shape[0]) \n",
        "\n",
        "\n",
        "    label_list = np.append(label_list, np.zeros(added_num))\n",
        "\n",
        "    corr_num = compute_correlation(original_data_list, original_label_list, fake_list)\n",
        "\n",
        "    #print(\"After Count: \", Counter(label_list))\n",
        "\n",
        "    return transformed_data_list, label_list, corr_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdkLD5dVRC-S"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloaders, dataset_sizes,  num_classes, epochs=1):\n",
        "    model = NN_Model(num_labels=num_classes)\n",
        "    \n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5, eps=1e-08) # clipnorm=1.0, add later\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #device = \"cpu\"\n",
        "    model.to(device)\n",
        "    \n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "#                 scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                #print(labels.long())\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    #print(labels.long())\n",
        "                    actual_labels = torch.max(labels.long(), 1)[1]\n",
        "                    loss = criterion(outputs, actual_labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "#                 running_loss += loss.item() * inputs.size(0)\n",
        "                running_loss += loss.item()\n",
        "                running_corrects += torch.sum(preds == actual_labels)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'validation' and epoch_loss < best_loss:\n",
        "#                 save_models(epoch,model)\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    #print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sjj0uOROROMY"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "  data_list = np.load('data_file.npy')\n",
        "  label_list = np.load('label_file.npy')\n",
        "\n",
        "\n",
        "  print(data_list.shape)\n",
        "\n",
        "  round_acc_arr = []\n",
        "  round_rec_arr = []\n",
        "  round_pre_arr = []\n",
        "  round_f1_arr = []\n",
        "  round_f1_micro_arr = []\n",
        "  round_corr_arr_train = []\n",
        "  round_corr_arr_test = []\n",
        "\n",
        "\n",
        "  round_acc_cum = 0\n",
        "  round_rec_cum = 0\n",
        "  round_pre_cum = 0\n",
        "  round_f1_cum = 0\n",
        "  round_f1_micro_cum = 0\n",
        "\n",
        "  round_range = 1000\n",
        "\n",
        "  #-------------------------------------------\n",
        "\n",
        "\n",
        "  for round_num in range(round_range):\n",
        "\n",
        "    print(\"Round Number: \", round_num)\n",
        "    acc_cum = 0\n",
        "    rec_cum = 0\n",
        "    pre_cum = 0\n",
        "    f1_cum = 0\n",
        "    f1_micro_cum = 0\n",
        "    acc_arr = []\n",
        "    rec_arr = []\n",
        "    pre_arr = []\n",
        "    f1_arr = []\n",
        "    f1_micro_arr = []\n",
        "    predicted_label_arr = []\n",
        "    test_label_arr = []\n",
        "    error_analysis = []\n",
        "    fold_number = 1\n",
        "\n",
        "    # Encode the Labels -------------------------------------------------------\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(label_list)\n",
        "    encoded_labels = encoder.transform(label_list)\n",
        "\n",
        "    class_weights_labels = class_weight.compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(encoded_labels),\n",
        "                                        y = encoded_labels                                                    \n",
        "                                    )\n",
        "    num_classes = len(list(encoder.classes_))\n",
        "    print(\"num_classes: \", num_classes)\n",
        "    print(encoder.classes_)\n",
        "\n",
        "    #-------------------------------------------------------\n",
        "    \n",
        "    # Transfer the labels to device\n",
        "    encoded_labels = np.asarray(encoded_labels, dtype='int32')\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #class_weights_labels = torch.tensor(class_weights_labels, dtype=torch.float, device=device)\n",
        "\n",
        "    for train_index, test_index in skf.split(data_list, encoded_labels):\n",
        "        print(\"Running fold #\", fold_number)\n",
        "        X_train, X_test = data_list[train_index], data_list[test_index]\n",
        "        y_train, y_test = encoded_labels[train_index], encoded_labels[test_index]\n",
        "        X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=42)\n",
        "        \n",
        "        # Apply SMOTE to Train, Test and Validation -----------------------------------------------\n",
        "        X_train, y_train, corr_train = apply_smote(X_train, y_train)\n",
        "        X_validation, y_validation, corr_validation = apply_smote(X_validation, y_validation)\n",
        "        X_test, y_test, corr_test = apply_smote(X_test, y_test)\n",
        "        #------------------------------------------------------------------------------------\n",
        "        \n",
        "        y_train = to_categorical(y_train)\n",
        "        y_validation = to_categorical(y_validation)\n",
        "        metric_test = np.copy(y_test)\n",
        "        y_test = to_categorical(y_test)\n",
        "\n",
        "        training_set = Dataset(X_train, y_train)\n",
        "        validation_set = Dataset(X_validation, y_validation)\n",
        "        test_set = Dataset(X_test, y_test)\n",
        "\n",
        "        dataloaders = {\n",
        "            'train' : torch.utils.data.DataLoader(training_set, batch_size=4,\n",
        "                                                 shuffle=True, num_workers=2, drop_last=True),\n",
        "            'validation' : torch.utils.data.DataLoader(validation_set, batch_size=4,\n",
        "                                                 shuffle=True, num_workers=2, drop_last=True)\n",
        "        }\n",
        "\n",
        "        dataset_sizes = {\n",
        "            'train': len(training_set),\n",
        "            'validation': len(validation_set),\n",
        "        }\n",
        "\n",
        "        print(len(training_set))\n",
        "        print(len(validation_set))\n",
        "        print(len(test_set))\n",
        "\n",
        "\n",
        "        model = train_loop(dataloaders, dataset_sizes, num_classes, epochs=35)\n",
        "        \n",
        "\n",
        "        y_pred = np.array([])\n",
        "\n",
        "        for i in tqdm(range(len(test_set))):\n",
        "            inputs = torch.Tensor([test_set[i][0]]).to(device)\n",
        "            model.eval()\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.max(outputs, 1)[1]\n",
        "            y_pred = np.append(y_pred, preds.cpu().numpy())\n",
        "\n",
        "        acc_arr.append(accuracy_score(metric_test, y_pred))\n",
        "        acc_cum += acc_arr[fold_number-1]\n",
        "        rec_arr.append(recall_score(metric_test, y_pred, average='macro'))\n",
        "        rec_cum += rec_arr[fold_number-1]\n",
        "        pre_arr.append(precision_score(metric_test, y_pred, average='macro'))\n",
        "        pre_cum += pre_arr[fold_number-1]\n",
        "        f1_arr.append(f1_score(metric_test, y_pred, average='macro'))\n",
        "        f1_cum  += f1_arr[fold_number-1]\n",
        "        f1_micro_arr.append(f1_score(metric_test, y_pred, average='micro'))\n",
        "        f1_micro_cum  += f1_micro_arr[fold_number-1]\n",
        "        fold_number+=1\n",
        "\n",
        "\n",
        "\n",
        "    round_acc_cum += acc_cum/5\n",
        "    round_acc_arr.append(acc_cum/5)\n",
        "\n",
        "    round_rec_cum += rec_cum/5\n",
        "    round_rec_arr.append(rec_cum/5)\n",
        "    \n",
        "    round_pre_cum += pre_cum/5\n",
        "    round_pre_arr.append(pre_cum/5)\n",
        "    \n",
        "    round_f1_cum += f1_cum/5\n",
        "    round_f1_arr.append(f1_cum/5)\n",
        "    \n",
        "    round_f1_micro_cum += f1_micro_cum/5\n",
        "    round_f1_micro_arr.append(f1_micro_cum/5)\n",
        "\n",
        "    #print(corr_train)\n",
        "    round_corr_arr_train.append(corr_train)\n",
        "    round_corr_arr_test.append(corr_test)\n",
        "\n",
        "\n",
        "\n",
        "  round_corr_arr_train = np.array(round_corr_arr_train)\n",
        "  round_corr_arr_test = np.array(round_corr_arr_test)\n",
        "\n",
        "  print(\"Train Correlation: \", np.mean(round_corr_arr_train,axis=0))\n",
        "  print(\"Test Correlation: \", np.mean(round_corr_arr_test,axis=0))\n",
        "\n",
        "  print(\"Accuracy: \", round_acc_cum/round_range)\n",
        "  print(\"Recall: \", round_rec_cum/round_range)\n",
        "  print(\"Precision: \", round_pre_cum/round_range)\n",
        "  print(\"F1 score(macro): \", round_f1_cum/round_range)\n",
        "  print(\"F1 score(micro): \", round_f1_micro_cum/round_range)\n",
        "\n",
        "  print(\"Accuracy_stdev: \", statistics.stdev(round_acc_arr))\n",
        "  print(\"Recall_stdev: \", statistics.stdev(round_rec_arr))\n",
        "  print(\"Precision_stdev: \", statistics.stdev(round_pre_arr))\n",
        "  print(\"F1(macro) score_stdev: \", statistics.stdev(round_f1_arr))\n",
        "  print(\"F1(micro) score_stdev: \", statistics.stdev(round_f1_micro_arr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIxpAt7eTtvw"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Neural_Network_Experiment.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}